############## PARAMS THAT NEED TO BE CHANGED FOR AMULET V100 VS. SANDBOX A100 ##############

### V100 ###
# Batch sizes
TRAIN_BATCH_SIZE: 2 # 8 per GPU for V100 (64 if using 8 GPUs)
VALIDATION_BATCH_SIZE: 2 # 32 per GPU for V100 (256 if using 8 GPUs)
TEST_BATCH_SIZE: 8 # 32 per GPU for V100 (256 if using 8 GPUs)

# Sampling
GRID_SAMPLER: false
IN_BATCH_SAMPLING: false # Use either in batch sampling or lable samples, not both.
WEIGHTED_SAMPLING: true #Sample rare sequences more often as dictaded by inverse frequency of labels
INV_FREQUENCY_POWER: 0.5 #Used for weighted sampling AND relevant losses with weighting
SEQUENCE_WEIGHT_AGG: sum #The aggregation to calculate sequence weight from applicable label weights.
SAMPLING_LOWER_CLAMP_BOUND: null # Lower bound for when clamping sampling weights
SAMPLING_UPPER_CLAMP_BOUND: null # Upper bound for when clamping sampling weights
TRAIN_LABEL_SAMPLE_SIZE: null # 15K for both. IS ALSO USED WITH GRID SAMPLER
VALIDATION_LABEL_SAMPLE_SIZE: null # use all labels in validation

# Inference
LABEL_BATCH_SIZE_LIMIT_NO_GRAD: 50 # 1K for V100
SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD: 32 # 64 for V100

# General
LEARNING_RATE: 0.0003
OPTIMIZER: Adam
WEIGHT_DECAY: 0.001 # Only used for ADAM W or SGD
PROTEIN_EMBEDDING_DIM: 1100
LABEL_EMBEDDING_DIM: 1024
LATENT_EMBEDDING_DIM: 1024
OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR: 3 # Scale MLP hidden state with respect to LATENT_EMBEDDING_DIM
OUTPUT_MLP_NUM_LAYERS: 3
OUTPUT_NEURON_PROBABILITY_BIAS: null
OUTPUT_MLP_BATCHNORM: true
RESIDUAL_CONNECTION: false
SYNC_BN: false
OUTPUT_MLP_DROPOUT: 0.0
LABEL_EMBEDDING_DROPOUT: 0.0
SEQUENCE_EMBEDDING_DROPOUT: 0.0
PROJECTION_HEAD_NUM_LAYERS: 4
PROJECTION_HEAD_HIDDEN_DIM_SCALE_FACTOR: 3
FEATURE_FUSION: concatenation # Select from concatenation, concatenation_diff, concatenation_prod, similarity
LABEL_EMBEDDING_POOLING_METHOD: mean # Select from mean, last_token, all
EXTRACT_VOCABULARIES_FROM: FULL_DATA_PATH # Can be any predefined path (e.g., FULL_DATA_PATH) or null. Null means generate vocab from scratch with dataset. Must set to null for zero-shot
OPTIMIZATION_METRIC_NAME: f1_macro # Only micro metrics are supported if sampling labels in validation
DECISION_TH_METRIC_NAME: f1_macro
ESTIMATE_MAP: false
NUM_EPOCHS: 46

# Memory optimizations
GRADIENT_ACCUMULATION_STEPS: 1 # 1 = no gradient accumulation
GRADIENT_CHECKPOINTING: false # True = gradient checkpointing
USE_AMP: true # Mixed precision (float16). Set false to avoid NaN with sequence encoder
LORA: true # True = use LORA
LORA_RANK: 4
LORA_ALPHA: 8 # Typically = 2 * Rank
CLIP_VALUE: 1 # Gradient clipping, set to null to turn off

# Losses. Only the parameters for selected loss will be used
LOSS_FN: FocalLoss # Currently supported: BCE, FocalLoss, BatchWeightedBCE, RGDBCE, WeightedBCE
FOCAL_LOSS_GAMMA: 2
FOCAL_LOSS_ALPHA: -1
BCE_POS_WEIGHT: 1 # 671.7130737304688
SUPCON_TEMP: 0.07
RGDBCE_TEMP: 0.12 # search from [1,1/3,1/5,1/7,1/9] according to paper
LABEL_SMOOTHING: 0.0 # Currently only implemented for FocalLoss

# Protein encoder settings
# Default: Hybrid ESM-C + EGNN encoder (use --use-sequence-encoder flag for legacy ProteInfer)
USE_PLM_EMBEDDINGS: true        # True: use ESM-C embeddings; False: use learned AA embeddings
LEARNED_AA_EMBEDDING_DIM: 128   # Dimension of learned AA embeddings (only used if USE_PLM_EMBEDDINGS=False)
EGNN_HIDDEN_DIM: 256            # EGNN hidden dimension
EGNN_OUT_DIM: 256               # EGNN output dimension (before projection)
EGNN_N_LAYERS: 4                # Number of EGNN layers
TRAIN_PROTEIN_ENCODER: true     # Whether to train the protein encoder

# Legacy ProteInfer encoder settings (only used with --use-sequence-encoder flag)
PRETRAINED_SEQUENCE_ENCODER: true
TRAIN_SEQUENCE_ENCODER: false  # Whether to train ProteInfer encoder
LABEL_ENCODER_NUM_TRAINABLE_LAYERS: 0
DISTRIBUTE_LABELS: false
TRAIN_PROJECTION_HEAD: true
LABEL_ENCODER_CHECKPOINT: intfloat/multilingual-e5-large-instruct #microsoft/biogpt, intfloat/e5-large-v2, intfloat/multilingual-e5-large-instruct

# Data processing and metrics
DEDUPLICATE: true
MAX_SEQUENCE_LENGTH: 10000 # Only affects training set. Just 8 seqs with 10K+ length
REMOVE_UNREPRESENTED_LABELS: false # Removes labels that never apply to train sequences
NORMALIZE_PROBABILITIES: false
INFERENCE_GO_DESCRIPTIONS: name+label # Ensembling during inference (validation, test). Only label, name, or name+label are supported.

# Augmentation
AUGMENT_RESIDUE_PROBABILITY: 0.1 # 0.0 = no augmentation. The probability that any given residue will be augmented. AlphaFold uses 0.15 (15% of residues are augmented)
USE_RESIDUE_MASKING: false # If True, also use masking to augment sequences. Must then also set AUGMENT_SEQUENCE_PROBABILITY > 0, and TRAIN_SEQUENCE_ENCODER = True
LABEL_AUGMENTATION_DESCRIPTIONS: name+label # Specifies what descriptions to use during training. # Options are any combination of label, name, and synonym_exact. Must include at least one.
LABEL_EMBEDDING_NOISING_ALPHA: 20.0 # 0.0 = no noising. Typical ranges are <20. Noising scalar from https://arxiv.org/pdf/2310.05914.pdf

# Interpretability (Phase 5): none = no interpretability loss; IntegratedGradient = IG + site ground truth
INTERPRETABILITY_METHOD: IntegratedGradient  # none | IntegratedGradient
INTERPRETABILITY_LOSS_TYPE: maximize  # mse = MSE(attribution, site target); maximize = maximize attribution on site atoms
INTERPRETABILITY_WEIGHT: 0.1   # weight for interpretability loss
INTERPRETABILITY_N_STEPS: 50   # number of steps for Integrated Gradients approximation (only for full IG)

# Constants
SEED: 42
EPOCHS_PER_VALIDATION: 1 # Must be >= 1
NUM_WORKERS: 3
DECISION_TH: 0.5 # Set to null if you want to use the best threshold from validation

# ESM-C Embeddings (for atom-level mode with --use-atom-level flag)
ESMC_MODEL_NAME: esmc_300m
ESMC_EMBEDDING_DIM: 960
ESMC_MATCH_SIZE: 8
KNN_K: 20

# Subset fractions (for rapid prototyping; set to 1 for final model)
TRAIN_SUBSET_FRACTION: 1 # Set to 1.0 if you want to use all data
VALIDATION_SUBSET_FRACTION: 1 # Set to 1.0 if you want to use all data
TEST_SUBSET_FRACTION: 1 # Set to 1.0 if you want to use all data
SHUFFLE_LABELS: false # Only has an effect if sample sizes are set and not using grid sampler. If False, simply select same labels in order
